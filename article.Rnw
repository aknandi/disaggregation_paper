\documentclass[article]{jss}

%% -- LaTeX packages and custom commands ---------------------------------------

%% recommended packages
\usepackage{thumbpdf,lmodern}

%% another package (only for this demo article)
\usepackage{framed}

%% new custom commands
\newcommand{\class}[1]{`\code{#1}'}
\newcommand{\fct}[1]{\code{#1()}}
\usepackage{amsmath}
\usepackage{subfig}
\usepackage{float}

%
<<kintroptions, echo=FALSE>>=
knitr::opts_chunk$set(
  collapse = TRUE,
  cache = TRUE,
  fig.width = 6,
  fig.height = 4,
  fig.pos = 'H',
  prompt=TRUE
)
options(prompt="R> ")
@
%

\author{Anita K Nandi\\University of Oxford
   \And Tim CD Lucas\\University of Oxford
   \And Rohan Arambepola\\University of Oxford
   \AND Peter Gething\\Telethon Kids Institute \\ Curtin University
   \And Daniel J Weiss\\University of Oxford}
\Plainauthor{Anita K Nandi, Tim CD Lucas, Rohan Arambepola, Peter Gething, Daniel J Weiss}

\title{\pkg{disaggregation}: An \proglang{R} Package for Bayesian Spatial Disaggregation Modelling}
\Plaintitle{disaggregation: An R Package for Bayesian Spatial Disaggregation Modelling}
\Shorttitle{Disaggregation Modelling in \proglang{R}}

\Abstract{
  Disaggregation modelling, or downscaling, has become an important discipline in epidemiology. Surveillance data, aggregated over large regions, is becoming more common, leading to an increasing demand for modelling frameworks that can deal with this data to understand spatial patterns. Disaggregation regression models use response data aggregated over large heterogenous regions to make predictions at fine-scale over the region by using fine-scale covariates to inform the heterogeneity. This paper presents the \proglang{R} package \pkg{disaggregation}, which provides functionality to streamline the process of running a disaggregation model for fine-scale predictions.
}

\Keywords{Bayesian spatial modelling, disaggregation modelling, TMB}
\Plainkeywords{Bayesian spatial modelling, disaggregation modelling, \pkg{TMB}}

\Address{
  Anita Nandi\\
  Malaria Atlas Project\\
  Big Data Institute\\
  Old Road Campus\\
  University of Oxford\\
  Oxford, UK\\
  E-mail: \email{anita.k.nandi@gmail.com}
}

\begin{document}
%\SweaveOpts{concordance=TRUE}

\section{Introduction} \label{sec:intro}

Methods for estimating high-resolution risk maps from aggregated response data over large spatial regions are 
becoming increasingly sought after in disease risk mapping~\citep{li2012log, diggle2013spatial, wilson2017pointless}, especially in malaria mapping~\citep{sturrock2014fine, sturrock2016mapping}. 
Disaggregation regression, first applied in species distribution modelling in ecology~\citep{keil2013downscaling} has now become an important method in disease risk mapping \citep{falciparum2019, vivax2019}. 
The aggregation of response data over large heterogenous regions is problematic for making fine-scale predictions, as relationships learned between variables at one spatial scale may not hold at other scales \citep{wakefield2006health}. However, by using fine-scale information from related covariates we can inform the hetrogeneity of the response variable of interest within the aggregated area.

Disaggregation modelling is unorthodox as the predictions are at a different scale to the response data, i.e., the number of rows in the covariate data is different to that of the response data. The spatial modelling software package, \pkg{INLA}~\citep{rue2009approximate}, or integrated nested Laplace approximation, has been shown to be very useful in a wide variety of circumstances, however it is not flexible enough for the unorthodox nature of the disaggregation problem except in the special case of the linear link function \citep{wilson2017pointless, moraga2017geostatistical}. Disaggregation models can be implemented in \pkg{TMB}~\citep{tmb2016}, or template model builder, with a lot of flexibility, however the data manipulation required to format the model objects and construct the model definition in C++ is non-trivial. The \pkg{disaggregation} package allows this process to be streamlined, to make it easy for the user to run disaggregation models at the expense of some flexibility.

\section{Disaggregation modelling} \label{sec:model}

Suppose we have response data, $y_i$, for $N$ polygons, which corresponds to count data for the property of interest within that polygon. 
The process that is being counted occurs in continuous space that we model as a high-resolution, square lattice for convenience.
The data, $y_i$, are assumed to be created by the aggregation of the counts over the polygon, i.e., the count data of the polygon is given by the sum of the count data for all the pixels within that polygon. An aggregation raster must be provided to transform from the pixel level predictions (rate) to count data, from which it is trivial to calculate the corresponding polygon value by summing up the values for the individual pixels within the polygon. For example, in epidemiology, we may have as our response the number of people that contract a certain disease in a given period of time (case incidence). Our rate would be the number of cases per population, where the aggregation raster corresponds to population. The case generating process is modeled at the pixel level, with these processes then aggregated to obtain the likelihood for the aggregated observed data.

For the disaggregation model we model the rate at pixel level, with the likelihood for the observed data given by aggregating these pixel level rates. The rate in pixel $j$ of polygon $i$ at location $s_{ij}$ is given by:
%
\begin{equation} \label{eq:lp}
\textrm{link}(\textrm{rate}_{ij}) = \beta_0 + \beta X_{ij} + \textrm{GP}(s_{ij}) + u_i 
\end{equation}
%
where $\beta$ are regression coefficients, $X_{ij}$ are covariate values, GP is a Gaussian random field and $u_i$ is a polygon-specific iid effect. The user-defined link function is typically log, identity and logit for Poisson, Normal and binomial likelihoods, respectively.  The Gaussian random field has a Matern covariance function, defined by:

%
\begin{equation}
Cov(d) = \frac{\sigma^2}{\Gamma(\nu)2^{\nu - 1}}(\kappa d)^{\nu}K_{\nu}(\kappa d)
\end{equation}
%

with two hyperparameters: $\rho = \frac{\sqrt{8\nu}}{\kappa}$, the range of the field (beyond which correlation is $<$ 0.1), and $\sigma$, the marginal standard deviation, relating to the magnitude of the variation in the field. The parameters $\rho$, $\sigma$ and $\nu$ are very difficult to identify together, therefore we fix $\nu$, to be able to identify $\rho$ and $\sigma$. The parameter $\nu$ is the smoothness and is fixed at 1, as it is considered the more natural basic choice for two-dimensional ($d=2$) models~\citep{INLA}. The Gamma function is given by $\Gamma(\nu) = (\nu - 1)!$ and $K_{\nu}$ is the modified Bessel function of the second kind.

As we are working in a Bayesian setting, each of the model parameters and hyperparameters are given a prior, which is discussed later. 

Incidence is modeled at the pixel level, and the response data exists as count data at the polygon level. Therefore, to calculate the likelihood we must aggregate the pixel rate, via the aggregation raster $a_{ij}$, to get the expected polygon count. This aggregation is defined by:

%
\begin{equation} \label{eq:agreggate1}
\textrm{cases}_i = \sum_{j=1}^{N_i} a_{ij}\textrm{rate}_{ij}
\end{equation}
\begin{equation} \label{eq:agreggate2}
\textrm{rate}_i = \frac{\textrm{cases}_{i}}{\sum_{j=1}^{N_i} a_{ij}}
\end{equation}
%
where $N_i$ is the total number of pixels in polygon $i$.
The different likelihoods correspond to slightly different models ($y_i$ is the response count data):

%
\begin{itemize}
\item \textbf{Poisson}
\begin{equation} \label{eq:poisson}
y_i | \beta_0, \beta_i, \textrm{GP}, u_i \sim \textrm{Poisson}(\textrm{cases}_i)
\end{equation}

\item {\textbf{Gaussian}
\begin{equation} \label{eq:gaussian}
y_i | \beta_0, \beta_i, \textrm{GP}, u_i \sim \textrm{Normal}(\textrm{cases}_i, \sigma_i)
\end{equation}
Here $\sigma_i = \sigma \sqrt{\sum_j a_{ij}^2}$, where  $\sigma$ is the pixel-level dispersion (a parameter learnt by the model) }

\item \textbf{Binomial}
\begin{equation} \label{eq:binomial}
y_i | \beta_0, \beta_i, \textrm{GP}, u_i \sim \textrm{Binomial}(M_i, \textrm{rate}_i)
\end{equation}

\end{itemize}
%
In the example of disease mapping, Poisson or Gaussian likelihoods could be used when the quantity observed, $y_i$, is the total number of cases in a given polygon. The binomial model could be used when $y_i$ is the prevalence of a disease in a sample of $M_i$ people in the polygon.

The pixel predictions of incident rate are calculated from the fitted model parameters using Equation~\ref{eq:lp}.

\subsection{Priors}

For each of the model parameters and hyperparameters we specify priors. The regression parameters and intercept are given Gaussian priors, where the default priors are $\beta_0 \sim N(0, 2)$ and $\beta_i \sim N(0, 0.4)$. It is expected that some of the spatial variation can be described by the covariates, and the Gaussian field can help describe the remaining spatial variation that is missing from the covariate information. Therefore, the priors on the covariates can be set to allow the covariates to explain some, but not all, of the variation in the response data. For the Gaussian random field, penalised complexity priors are used, which are constructed to  penalise against deviating from the simpler base model, which in this case is a flat field, i.e. zero variance and infinite range~\citep{fuglstad2018constructing}. A penalised complexity prior is placed on the scale and range parameters of the random field such that
\begin{align}
    P(\rho < \rho_{min}) &= \rho_{prob} \\
    P(\sigma > \sigma_{max}) &= \sigma_{prob}
\end{align}
where the values $\rho_{min}, \,\rho_{prob}, \,\sigma_{max}, \,\sigma_{prob}$ are set by the user. The default values for these parameters within the package are driven by the nature of the data provided in the model. The default prior for $\rho_{min}$ is set at a third of the spatial area covered by the polygons, and $\rho_{prob}$ is set at 0.1. The default prior for $\sigma_{max}$ is set to the standard deviation of the normalised response data, and $\sigma_{prob}$ is set at 0.1. 

The joint penalised complexity prior, $\pi$, corresponding to a base model with infinite range and zero variance~\citep{fuglstad2018constructing} is given by:

%
\begin{equation} 
log\left(\pi(\sigma, \rho)\right) = log\left(\tilde{\lambda_1}\right) +  log\left(\tilde{\lambda_2}\right) - 2log(\rho) - \frac{\tilde{\lambda_1}}{\rho} - \tilde{\lambda_2} \sigma
\end{equation}

where

\begin{equation} 
\tilde{\lambda_1} = -\rho_{min}log(\rho_{prob}) \qquad \text{and} \qquad \tilde{\lambda_2} = -\frac{log(\sigma_{prob})}{\sigma_{max}} 
\end{equation}
%

This prior shrinks the field towards a base model with zero variance and infinite range, in other words regularising towards a flatter field with smaller magnitude. 

The polygon-specific effects $u_1,...,u_N$ have Gaussian priors centered at 0 with standard deviation $\sigma_{u}$ (where the precision $\tau_{u} = 1/\sigma_{u}^2$). A penalised complexity prior is placed on $\sigma_{u}$~\citep{simpson2017penalising} such that 
\begin{equation}
P(\sigma_{u} > \sigma_{u, max}) = \sigma_{u, prob}
\end{equation}
where values $\sigma_{u, max}$ and $\sigma_{u, prob}$ are set by the user.
The penalised complexity prior, $\pi_u$, corresponding to a base model with no polygon-specific effect~\citep{simpson2017penalising} is given by:

%
\begin{equation} 
log\left(\pi(\lambda)\right) = log\left(\frac{\lambda}{2}\right) - \frac{3}{2}log(\tau_u) - \frac{\lambda}{\sqrt{\tau_u}} 
\end{equation}

where

\begin{equation} 
\lambda = -\frac{log(\sigma_{u, prob})}{\sigma_{u, max}} \qquad \text{and} \qquad \tau_u = \frac{1}{\sigma_u^2}
\end{equation}
%

This prior shrinks towards a base model of no polygon-specific effect. 

For models that use a Gaussian likelihood, a log gamma prior is set on the log of the precision, $\log\tau_u \sim \log\Gamma(\text{shape} = 1, \text{rate} = 5 \times 10^{-5})$, to regularise the dispersion, $\sigma_u$, to take low values. This is chosen to be consistent with the prior set by INLA for the dispersion of the normal likelihood. We aim for consistency with INLA as INLA is one of the most commonly used packages for Bayesian spatial modeling and due to the parallels in computation approach such as the use of a Laplace approximation and the SPDE approximation to the Gaussian Random Field.


\section{Implementation} \label{sec:implementation}

The \pkg{disaggregation} package is built on template model builder (\pkg{TMB})~\cite{tmb2016}, which is a tool for flexibly building complex models based on C++. \pkg{TMB} combines the packages CppAD~\citep{bell2012cppad}, for automatic differentiation in C++, Eigen~\citep{guennebaud2010eigen}, a C++ library for linear algebra, and CHOLMOD~\citep{chen2008algorithm}, a package for efficient computation of sparse matrices. The use of these packages allows an efficient implementation of the automatic Laplace approximation~\citep{skaug2006automatic} with exact derivatives which gives an approximation to the Bayesian posterior. \pkg{TMB} calculates first and second order derivatives of the objective function using automatic differentiation~\citep{griewank2008evaluating}.

TMB is a package which allows the user to define and fit latent variable models. The user defines the objective function (typically the likelihood or posterior density function) in C++ and TMB then implements the Laplace approximation to integrate out random effects and generates functions for evaluating the objective and gradient (via automatic differentiation). These functions can then be passed to an optimiser (for maximum likelihood estimation or to find the posterior maximum) or sampler (for MCMC) in R. \pkg{TMB} can fit both Frequentist and Bayesian models. In this case we have chosen the Bayesian approach as it is most widely used in the spatial modelling community.

The \pkg{disaggregation} package contains a C++ function that defines the model and computes the joint likelihood as a function of the parameters and the random effects, in the format expected by \pkg{TMB}. The \pkg{TMB} package then calculates estimates of both parameters and random effects using a Laplace approximation. Evaluation of the objective function and its derivatives is performed via \proglang{R}.


\subsection{Other implementations}


The simplest method to disaggregate data from the polygon level to the high-resolution pixel level is to simply apply the value given by the polygon across all pixels within that polygon.
This is the approach used by the \code{rasterize} function in the \pkg{raster} package, the \code{fasterize} function in the \pkg{fasterize} package and \code{st_interpolate_aw} from the \pkg{sf} package \citep{raster, fasterize, sf}.
Similarly, for time-series data there are functions such as \code{td} in the \pkg{tempdisagg} package \citep{tempdisagg}.
These methods are not based on a statistical model and cannot help estimate the heterogenous distribution of the disease within a polygon.
Furthermore, these methods cannot make predictions outside of the area for which polygon data is available.

A number of other implementations use statistical models to estimate high resolution disease risk from aggregated disease data.
To the best of our knowledge, these all only include covariates at the polygon level rather than at the pixel level.
The popular R package \pkg{INLA} can perform disaggregation regression in the simplest case of a normal likelihood, an identity link function and no high resolution covariates \citep{moraga2017geostatistical, INLA, wilson2017pointless}. 
The \pkg{SDALGCP} package fits the model using Monte Carlo Maximum Likelihood \citep{johnson2019spatially}.
It can only fit models with a Poisson likelihood with covariates at the polygon level.
Finally, the \pkg{lgcp} package uses a data augmentation approach to fit models to aggregated data \citep{lgcp}.  
This Monte Carlo data augmentation approach will be very slow for large areas or large numbers of cases.

In this paper we compare our method to Markov chain Monte Carlo (MCMC). MCMC methods involve constructing Markov chains which (after a sufficient warm up period) produce samples from the desired probability distribution. In high dimensional problems, these methods can be extremely slow as many steps are needed to converge to and effectively sample from the target distribution. This is particularly problematic when computing the likelihood is costly, as the likelihood is typically evaluated at each step. In spatial settings with a Gaussian field, MCMC becomes infeasible for disaggregation modelling. Our method presented in the paper leverages the Laplace approximation to provide a much faster way of fitting disaggregation models. 


\section{Package usage} \label{sec:code}

In this section we show how to use the \pkg{disaggregation} package using a dataset of aggregated malaria case counts across Madagascar in 2016. Malaria is an infectious disease caused by parasites of the Plasmodium group, transmitted by Anopheles mosquitoes. Malaria transmission is therefore closely related to mosquito and parasite development. Environmental factors such as temperature, rainfall and elevation have been shown to have signifcant effects on mosquito survival and development; in general mosquitoes favour warm, humid environments with moderate rainfall. Therefore, such environmental covariates would be useful in a malaria disaggregation model to inform fine-scale distributions. In this model we use the environmental covariates of mean land surface temperature (LST), elevation, and enhanced vegetation index (EVI), at a resolution of approximately $5\times5~km$, to inform spatial heterogeneity in malaria risk. These covariates are obtained from the Moderate Resolution Imaging Spectroradiometer (MODIS), which provides many measurements over the entire Earth's surface (\url{https://modis.gsfc.nasa.gov/data/}).

Malaria incidence rate is often given per thousand people per year given the term annual parasite index (API).
In this case our pixel predictions correspond to malaria incidence rate, so we use population to aggregate pixel incidence rate by summing the number of cases (rate weighted by population). Raster surfaces of population for the years 2010 and 2015 at a resolution of approximately $5\times5~km$, were created using data from WorldPop~\citep{tatem2017worldpop} and from GPWv4~\citep{GpwNasa} where WorldPop did not have values. The population raster for 2016 was created by linear interpolation between 2010 and 2015 and extending out to 2016. This interpolation method results in a small amount of uncertainty in population raster, however this is expected to be negligible compared to the model uncertainty.

Covariate rasters or population rasters may be accompanied by significant uncertainty. There is no robust way within the package to propagate this uncertainty. However, if the user were able to sample from the covariate rasters and run many instances of the model, they could estimate an overall uncertainty. Whether this is a good approximation of the true uncertainty will depend on the specific use case.

The covariate rasters and aggregation raster provided must be of the same spatial scale and must be spatially aligned. For spatially misaligned rasters, there are packages in \prolang{R} to align rasters such as \code{align_rasters} in the \pkg{gdalUtils} package~\cite{gdalutils}.

The latest version of \pkg{disaggregation} should always be available from the Comprehensive R Archive Network (CRAN) at \href{http://CRAN.R-project.org/package=disaggregation}{http://CRAN.R-project.org/package=disaggregation}. Run the following commands to install and load the package.

%
<<installrun, echo=FALSE, message=FALSE, warnings=FALSE>>=
library(disaggregation, quietly = TRUE)
library(raster, quietly = TRUE)
@
%

%
<<installview, eval=FALSE>>=
devtools::install.packages("disaggregation")
library(disaggregation)
@
%

We then read in the data to use in the \pkg{disaggregation} package. The \code{shapefile} function and \code{raster} function are contained within the \pkg{raster} package, they are functions to read spatial data and raster data respectively. These functions return objects of type SpatialPolygonsDataFrame and RasterLayer respectively. A SpatialPolygonsDataFrame is an \prolang{R} class that holds spatial data in the form of polygons with attributes, in this case the attributes are number of malaria cases within the polygon and polygon ID. A RasterLayer is an \prolang{R} class that holds a single raster. The function \code{getCovariateRasters} is a helper function contained within the \pkg{disaggregation} package. From a user defined a directory name and shapefile, all the raster files within that directory are read and a RasterStack is created with the same extent as the shapefile provided. In this example we are building a RasterStack of the covariate rasters with the same extent as the population raster. A RasterStack is a collection of RasterLayer objects with the same spatial extent and resolution.

%
<<setuppaths, results='hide', show='hide'>>=
library(raster)

shapes <- shapefile('data/shapes/mdg_shapes.shp')
population_raster <- raster('data/population.tif')
covariate_stack <- getCovariateRasters('data/covariates', 
                                       shape = population_raster)
@
%

The main functions are \code{prepare_data}, \code{disag_model} and \code{predict}.

Firstly, we use the \code{prepare_data} function to setup all the data in the format needed in the disaggregation modelling. This function performs various data manipulation tasks to create objects that are necessary for fitting the model. The required input data for the \code{prepare_data} function are:
\begin{itemize}
\item \code{polygon_shapefile}: SpatialPolygonsDataFrame containing the response data. It must contain IDs and response data.
\item \code{covariate_raster}: RasterStack of covariates to be used in the model.
\item \code{aggregation_raster}: RasterLayer used as the weights to aggregate the pixel values within a polygon.
\item \code{mesh.args}: List of parameters to control the mesh used for the Gaussian Field component.
\item \code{id_var}: the variable name of the ID variable in the \code{polygon_shapefile}.
\item \code{response_var}: the variable name of the response variable in the \code{polygon_shapefile}.
\item \code{na.action}: Boolean. Whether to deal with NAs or not.
\item \code{ncores}: Number of cores to perform the parallel extraction over.
\end{itemize}

An optional aggregation raster can be provided. The aggregation raster defines how the pixels within each polygon are aggregated. The disaggregation model performs a weighted sum of the pixel predictions, weighted by the pixel values in the aggregation raster, as shown in~Equation~\ref{eq:agreggate1}. In this case our pixel predictions are malaria incidence rate, so we use the population raster to aggregate pixel incidence rate by summing the number of cases (rate weighted by population). If no aggregation raster is provided a uniform distribution is assumed, i.e., the pixel predictions are aggregated to polygon level by summing the pixel values unaltered.

The values of the covariates (as well as the aggregation raster, if given) are extracted at each pixel within the polygons and stored as a data.frame with a row for each pixel and a column for each covariate (\code{parallelExtract} function). The extraction of each covariate is performed in parallel over the number of cores defined by the argument \code{ncores}. The values extracted from the aggregation raster are returned as an array of values, one for each pixel. In order to know which pixels (i.e., which rows) are contained in each polygon, a matrix is constructed that contains the start and end pixel index for each polygon (\code{getStartendindex} function).

To fit a model with a Gaussian field we approximate a Gaussian field using Gaussian Markov Random Fields (GMRF), and solve using the stochastic partial differential equation (SPDE) approach~\citep{Lindgren2011}. This approach requires building a mesh, i.e. splitting the space into finite elements, and calculating field values at the mesh nodes. If a mesh is too fine, the field values take too long to compute, however if the mesh is too coarse, the resulting field is a poor approximation and leads to mesh artifacts in the predictions. The 2D mesh for the spatial field is built using the \code{build_mesh} function, which makes use of the \pkg{INLA} function \code{inla.mesh.2d}. The user can control the parameters of the mesh, including the granularity, using the argument \code{mesh.args} in the \code{build_mesh} function. The parameters \code{max.edge}, \code{cut} and \code{offset} that can be set in the \code{mesh.args} list are defined within the \code{inla.mesh.2d} function. By providing two values to the \code{max.edge} parameter, the mesh contains an inner region of finer mesh, and an outer coarse region. This approach allows the region of interest to have a fine as mesh as necessary without also requiring time consuming computations in the sea regions. The mesh can take several minutes to construct, so to prepare the data without building the mesh the user can set the \code{makeMesh} flag to FALSE. However, it would then not be possible to fit the disaggregation model without the mesh. 

If there are any NAs in the response or covariate data within the polygons the \code{prepare_data} method will return an error. This can be dealt with using the \code{na.action} flag, which is automatically off.  Ideally the NAs in the data would be dealt with by the user beforehand, however, setting na.action = TRUE will automatically deal with NAs. It removes any polygons that have NAs as a response, sets any aggregation pixels with NA to zero and sets covariate NA pixels to the median value for that covariate across all polygons.

%
<<prepare_data, warning = FALSE>>=
dis_data <- prepare_data(polygon_shapefile = shapes, 
                         covariate_rasters = covariate_stack, 
                         aggregation_raster = population_raster, 
                         mesh.args = list(max.edge = c(0.7, 8), 
                                          cut = 0.05, 
                                          offset = c(1, 2)),
                         id_var = 'ID_2',
                         response_var = 'inc',
                         na.action = TRUE,
                         ncores = 8)
@
%

We can see a summary of the data, using the generic \code{summary} function, and \code{plot} the data. The summary function returns information on how many pixels and polygons the data contains, how many pixels in the smallest and largest polygons and a summary of the covariate data. The \code{plot} functions plots a map of the polygon response data, the covariate rasters and the INLA mesh, as shown in Figure~\ref{fig:plotdata}.

%
<<summarydata>>=
summary(dis_data)
@
%

\begin{figure}[!h]
\centering
%
<<plotdata, fig.width = 15, fig.height = 15>>=
plot(dis_data)
@
%
\caption{\label{fig:plotdata} Maps of Madagascar showing the data used in the disaggregation model. These plots are produced when calling the \code{plot} function on the \code{disag\_data} object. The plots show the number of malaria cases in each administrative unit (top left), maps of the four covariates used in the model (top right), and inla.mesh object that will be used to make the spatial field (bottom).}
\end{figure}

The \code{prepare_data} function returns an object of class \code{disag_data}, which is designed to be used directly in the \code{disag_model} function.

Now we can fit the disaggregation model using \code{disag_model}. The required inputs for the \code{disag_model} function are:
\begin{itemize}
\item \code{data}: Object of class \code{disag_data} returned by the \code{prepare_data} function.
\item \code{priors}: List of priors and hyperpriors to use for the model. For any not set, the default priors will be used.
\item \code{iterations}: Maximum number of iterations the model can run for to find an optimal point.
\item \code{family}: Likelihood function. Options are gaussian, poisson and binomial.
\item \code{link}: Link function used in the model. Options are logit, log and identity. This would typically be log, identity and logit for Poisson, Normal and binomial likelihoods, respectively.
\end{itemize}

Here we use a Poisson likelihood for the incidence count data with a log link function. The spatial field and iid effect are components of the model by default, they can be turned off using the \code{field} and \code{iid} flags. In this example we use the default priors. Hyperpriors for the field are implemented as penalised complexity priors - specify $\rho_{min}$ (\code{prior_rho_min}) and $\rho_{prob}$ (\code{prior_rho_prob}) for the range of the field, where $P(\rho < \rho_{min}) = \rho_{prob}$, and $\sigma_{max}$ (\code{prior_sigma_max}) and $\sigma_{prob}$ (\code{prior_sigma_prob}) for the variation of the field, where $P(\sigma > \sigma_{max}) = \sigma_{prob}$. Similarly, penalised complexity priors are implemented for the iid effect (\code{prior_iideffect_sd_max} and \code{prior_iideffect_sd_prob}). Any of the priors and hyperpriors can be set in the list of priors given to the \code{priors} argument of the \code{disag_model} function. In order to print more verbose output the user can set the \code{silent} argument to FALSE.

%
<<disag_model, message = FALSE, warnings=FALSE>>=
fitted_model <- disag_model(data = dis_data,
                            iterations = 1000,
                            family = 'poisson',
                            link = 'log')
@
%

We can get a summary and plot of the model output. The \code{summary} function gives the estimate and standard error of the fixed effect parameters in the model, as well as the negative log likelihood and in-sample performance metrics (root mean squared error, mean absolute error, pearson correlation coefficient and spearman rank correlation coefficient). The standard error is calculated based on the approximate posterior from the \pkg{TMB} package. The \code{plot} function produces two plots: one of the fixed effects parameters and one of the observed data against in-sample predictions, as shown in Figure~\ref{fig:plotmodel}.

%
<<summarymodel>>=
summary(fitted_model)
@
%

\begin{figure}[!h]
\centering
%
<<plotmodel, fig.width = 13, fig.height = 6>>=
plot(fitted_model)
@
%
\caption{\label{fig:plotmodel} Plot summarising the results of the fitted model. These plots are produced when calling the \code{plot} function on the \code{disag\_model} object. The fixed effects plot (left) shows the fitted parameter values with uncertainty estimation for all the fixed effects in the model. The in-sample performance plot (right) shows the predicted incidence rate values for each polygon in the data against the observed values for that polygon in the data.}
\end{figure}

The \code{disag_model} function returns an object of class \code{disag_model}, which is designed to be used directly in the \code{predict} function.
Therefore, now that we have fitted the model, we are ready to predict the malaria incidence rate across Madagascar.

To predict over a different spatial extent to that used in the model, a RasterStack covering the region to make predictions over can be passed as the \code{newdata} argument. If this argument is not given, predictions are made over the covariate rasters used in the fit. If the user wants to include the iid effect from the model as a component in the prediction then the \code{predict_iid} logical flag should be set to TRUE, otherwise, the iid effect will not be predicted. 

For the uncertainty calculations, parameter values are sampled from the posterior distribution and summarised. The number of parameter draws used to calculate the uncertainty is set by the user via the \code{N} parameter (default: 100), and the size of the credible interval (e.g., 75\%, 95\%) to be calculated when summarising is set via the argument \code{CI} (default: 0.95). 

%
<<predict_model>>=
model_prediction <- predict(fitted_model)
@
%

The function \code{predict} returns a object of class \code{disag_prediction} containing a list of two objects: the mean predictions and the uncertainty rasters. The mean predictions contain a raster of the mean prediction of the incidence rate, as well as rasters of the field, iid (if predicted) and covariate component of the linear predictor. The uncertainty predictions contains a RasterStack of the prediction realisations and a RasterStack of the upper and lower credible intervals. 

The \code{plot} function can be used on the \code{disag_prediction} objects, as shown in Figure \ref{fig:plotprediction}. From Figure~\ref{fig:plotdata}, it can be seen that the polygon response data does not include the islands, whereas the covariate rasters do. As can be seen in Figure~\ref{fig:plotprediction}, out-of-sample malaria predictions of these islands have been made, however these predictions do not contribute to the in-sample performance in Figure~\ref{fig:plotmodel}, nor to cross validation performance. In fact, these islands are not part of Madagascar at all, they are the islands of Comoros and Mayotte.

\begin{figure}
\centering
%
<<plotprediction, dev = 'png', fig.width = 15, fig.height = 10>>=
plot(model_prediction)
@
%
\caption{\label{fig:plotprediction} Maps of Madagascar showing the fine-scale predictions of mean, lower (2.5\%) and upper (97.5\%) credible intervals of the malaria incidence rate from the disaggregation model. These maps are produced when calling the \code{plot} function on the \code{disag\_prediction} object.}
\end{figure}

Using three simple functions within the \pkg{disaggregation} package we have been able to fit a Bayesian spatial disaggregation model and predict pixel-level incidence rate across Madagascar using aggregated incidence data and pixel-level environmental covariates.

The same technique can be used for spatio-temporal disaggregation modelling. In order to achieve this, dynamic covariates are required, for example, annual covariate rasters, as well as a spatio-temporal field, which could be achieved by adding a time varying autoregressive component to the spatial field. A difficulty with this approach is that it results in significant increase in computational complexity of the optimisation problem, most notably from the spatio-temporal field, leading to a significant increase in the time taken to perform the optimisation. The package can be developed to include spatio-temporal disaggregation models, however that is beyond the scope of this paper.

\newpage
\section{Comparison with Markov chain Monte Carlo (MCMC)} \label{sec:comparison}

In this section we show performance comparisons between modelling using the Laplace approximation provided by the \pkg{disaggregation} package, based on \pkg{TMB}, and using MCMC. Given a function to evaluate the probability density of a distribution at any given point in parameter space, MCMC algorithms construct markov chains to generate samples from this distribution. These algorithms are often slow, particularly in high-dimensional settings, as it can take a long time to converge to and effectively sample from the stationary distribution. The density is evaluated at each step, so this problem is compounded when evaluating this density is computationally expensive. In contrast, the \pkg{disaggregation} package approximates using a Laplace approximation to the posterior to generate posterior samples. This only requires the posterior to be maximised to find the posterior mode and therefore involves relatively few evaluations of the posterior density compared to MCMC techniques, although potentially at the expense of less accurate posterior samples. Here we compare the time and performance of the two techniques.

The model described in Section~\ref{sec:code} for malaria in Madagascar has been optimised using the \pkg{disaggregation} package. Here we fit the same model by running MCMC using the \pkg{tmbstan} package, with the NUTS (no-u-turn sampler) algorithm \citep{hoffman2014no} using four chains. It is important to note that this is a useful feature of the \pkg{disaggregation} package, to be able to create the \pkg{TMB} model object (using the \code{disag_model} function with one iteration) and pass it directly to \pkg{tmbstan}. The model is fitted by running the MCMC algorithm for 8000 iterations with 2000 of those as warmup, which took 74 hours. This number of iterations was chosen by running the MCMC algorithm repeatedly, starting at 1000 iterations, doubling the number of iterations each time until the value of the MCMC convergence statistic, $\hat{R}$, dropped below 1.05 for all model parameters. The property $\hat{R}$ is known as the Gelman-Rubin diagnostic~\citep{Rhat}, and is commonly used to evaluate MCMC convergence across multiple chains. In contrast, fitting the model using the Laplace approximation via \pkg{TMB} within the \pkg{disaggregation} package took 56 seconds.

Fitted parameter values for both of these methods are given in Table~\ref{tab:mcmcresults}. It can be seen that the model parameters are very similar between the two methods. It is worth noting that the field parameters given in the Table are $log(\rho)$ and $log(\sigma)$. If we transform these parameters to their natural form, the values given in Table~\ref{tab:mcmcresults} correspond to a difference in the range of the field, $\rho$, from $1.9 \pm 0.6$ (TMB) to $2.3 \pm 0.8$ (MCMC), and a difference in the standard deviation of the field, $\sigma$, from $1.00 \pm 0.17$ (TMB) and $1.07 \pm 0.21$ (MCMC). Considering these values, along with their large accompanying uncertainties, these field predictions are very similar between the two methods. In general, the Laplace approximation will perform worse when the posterior is less Gaussian; this occurs when the prior has more influence in the model, for example, when there is less data or for hierarchical parameters.

%
<<mcmc, eval=FALSE>>=
library(tmbstan)

model_object <- make_model_object(data = dis_data,
                                  family = 'poisson',
                                  link = 'log')

start <- Sys.time()
mcmc_out <- tmbstan(model_object, chains = 4, iter = 8000, warmup = 2000,
                    cores = getOption('mc.cores', 4))
end <- Sys.time()
print(end - start)

stan_trace(mcmc_out, 
           pars = c('intercept', 
                    'slope[1]', 'slope[2]', 'slope[3]',
                    'iideffect_log_tau', 'log_sigma', 'log_rho'))
@
%

%
\begin{table}
\centering
\begin{tabular}{l|ll|ll}
 & \multicolumn{2}{|c|}{MCMC (74 hours)} & \multicolumn{2}{|c}{TMB (56 seconds)} \\
Parameter & Mean & SD & Mean & SD \\
\hline
Intercept & -3.23 & 0.31 & -3.20 & 0.27 \\
Slope 1 & -0.47 & 0.20 & -0.47 & 0.20 \\
Slope 2 & 0.41 & 0.23 & 0.39 & 0.22 \\
Slope 3 & 0.21 & 0.31 & 0.21 & 0.29 \\
$log(\tau_u)$ & 0.92 & 0.28 & 0.98 & 0.27 \\
$log(\sigma)$ & 0.07 & 0.20 & -0.002 & 0.166 \\
$log(\rho)$ & 0.82 & 0.35 & 0.63 & 0.33 \\
\end{tabular}
\caption{Fitted model parameter values using both MCMC and using \pkg{TMB} within the \pkg{disaggregation} package.}
\label{tab:mcmcresults}
\end{table}
%

The trace of the MCMC parameter values is given in Figure~\ref{fig:mcmclong}. It can be seen that the MCMC algorithm has been run for long enough to get sufficient chain mixing. The \pkg{disaggregation} package produces similar results to the MCMC algorithm. However the model fitting using the \pkg{disaggregation} package took 56 seconds in contrast to the 74 hours taken for the MCMC run. Therefore, it can be seen that the \pkg{disaggregation} package provides a quick and simple way to run disaggregation models, that can be prohibitively slow using MCMC.

%
\begin{figure}[t!]
\centering
\includegraphics[width=\linewidth]{figures/mcmc_trace.pdf}
\caption{Trace of the fixed effects parameters for MCMC using NUTS sampling, running the algorithm for 74 hours. It was run for 8000 iterations with 2000 of those as warmup.}
\label{fig:mcmclong}
\end{figure}
%

%% -- Summary/conclusions/discussion -------------------------------------------

\section{Conclusions} \label{sec:conclusions}

Disaggregation modelling, which involves fitting models at fine-scale resolution using areal data over heterogenous regions, has become widely used in fields such as epidemiology and ecology. The \pkg{disaggregation} package implements Bayesian spatial disaggregation modelling with a simple, easy to use \proglang{R} interface. The package includes simple data preparation, fitting and prediction functions that allow some user-defined model flexibility. In this paper we have presented an application of the package, predicting malaria incidence rate across Madagascar from aggregated count data and environmental covariates. 

The modelling framework is implemented using the Laplace approximation and automatic differentiation within the \pkg{TMB} package. This allows fast, optimised calculations in C++. These disaggregation models are computationally intensive and take a long time using MCMC optimisation techniques. Using \pkg{TMB}, the models are much faster and produce similar results.

Future work could be done extending the \pkg{disaggregation} package to include spatio-temporal disaggregation models. This would require a spatio-temporal field as well as dynamic covariates, and would be significantly more computationally intensive. Additionally, tools for cross-validation could be included within the package. Cross-validation of spatial models is non-trivial due to the spatial autocorrelation in the data. 

The \pkg{disaggregation} package provides a simple, useful interface to perform spatial disaggregation modelling, with reasonable flexibility, as well as having the scope to be extended to more complex disaggregation models.


%% -- Optional special unnumbered sections -------------------------------------

\section*{Computational details}

The results in this paper were obtained using
\proglang{R}~\Sexpr{paste(R.Version()[6:7], collapse = ".")} with the
\pkg{TMB}~\Sexpr{packageVersion("TMB")} package. \proglang{R} itself
and all packages used are available from the Comprehensive
\proglang{R} Archive Network (CRAN) at
\url{https://CRAN.R-project.org/}, apart from \pkg{INLA}, which can be 
installed in \proglang{R} using the command:
%
<<inla, eval=FALSE>>=
install.packages("INLA", 
                 repos=c(getOption("repos"), 
                         INLA="https://inla.r-inla-download.org/R/stable"), 
                 dep=TRUE)
@
%


\section*{Data availability}

All data used in this paper, including the results of the MCMC algorithm, can be found here: \url{https://figshare.com/projects/disaggregation/96095}.


\section*{Acknowledgments}

We would like to thank the Bill and Melinda Gates Foundation for funding this research.

\bibliography{refs}

\end{document}
